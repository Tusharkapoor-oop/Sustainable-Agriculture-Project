{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380ab06c-f841-4c74-9087-6b2397fab01b",
   "metadata": {},
   "source": [
    "# src/data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec7afbfe-ea39-42fe-b2f2-ee6bf945a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data_preprocessing\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17dbe8cb-49c1-48db-a3e6-7cc60246b304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: C:\\Users\\tusha\n",
      "Raw data dir: C:\\Users\\tusha\\data/raw\n",
      "Processed data dir: C:\\Users\\tusha\\data/processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "BASE_DIR = os.getcwd()  \n",
    "RAW_DIR = os.path.join(BASE_DIR, \"data/raw\")\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, \"data/processed\")\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Base directory:\", BASE_DIR)\n",
    "print(\"Raw data dir:\", RAW_DIR)\n",
    "print(\"Processed data dir:\", PROCESSED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66229f7a-d162-4825-9dc8-7d2355878cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Basic preprocessing utils\n",
    "# -----------------------\n",
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Fill or drop missing values in the dataset.\"\"\"\n",
    "    for col in df.select_dtypes(include=\"number\").columns:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def scale_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Scale numerical features using StandardScaler.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Main workflow\n",
    "# -----------------------\n",
    "def load_data():\n",
    "    \"\"\"Load crop, soil, and weather datasets.\"\"\"\n",
    "    try:\n",
    "        crop_df = pd.read_csv(os.path.join(RAW_DIR, \"Crop_recommendation.csv\"))\n",
    "        soil_df = pd.read_csv(os.path.join(RAW_DIR, \"data_core.csv\"))\n",
    "        weather_df = pd.read_csv(os.path.join(RAW_DIR, \"weatherHistory.csv\"))\n",
    "\n",
    "        print(\"‚úÖ Datasets loaded successfully!\")\n",
    "        return crop_df, soil_df, weather_df\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(f\"‚ùå Missing file! Check RAW folder. Details: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc24f7fa-cb0a-4ca2-b8b9-0309573e7357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_and_merge():\n",
    "    \"\"\"Merge crop, soil, and weather data into a final dataset.\"\"\"\n",
    "    crop_df, soil_df, weather_df = load_data()\n",
    "\n",
    "    # Merge Crop + Soil\n",
    "    common_cols = list(set(crop_df.columns) & set(soil_df.columns))\n",
    "    if common_cols:\n",
    "        crop_soil_df = pd.merge(crop_df, soil_df, on=common_cols, how=\"inner\")\n",
    "        print(f\"üîó Crop + Soil merged on columns: {common_cols}\")\n",
    "    else:\n",
    "        crop_soil_df = pd.concat([crop_df, soil_df], axis=1)\n",
    "        print(\"‚ö†Ô∏è No common keys between Crop and Soil. Using concatenation.\")\n",
    "\n",
    "    # Merge with Weather\n",
    "    common_weather_cols = list(set(crop_soil_df.columns) & set(weather_df.columns))\n",
    "    if common_weather_cols:\n",
    "        final_df = pd.merge(crop_soil_df, weather_df, on=common_weather_cols, how=\"left\")\n",
    "        print(f\"üå¶Ô∏è Crop+Soil + Weather merged on: {common_weather_cols}\")\n",
    "    else:\n",
    "        final_df = crop_soil_df\n",
    "        print(\"‚ö†Ô∏è No common keys with Weather dataset. Skipping merge.\")\n",
    "\n",
    "    # Handle missing values\n",
    "    final_df = final_df.ffill().bfill().infer_objects(copy=False)\n",
    "\n",
    "    # Save\n",
    "    processed_file = os.path.join(PROCESSED_DIR, \"final_dataset.csv\")\n",
    "    final_df.to_csv(processed_file, index=False)\n",
    "    print(f\"‚úÖ Final dataset saved at: {processed_file}\")\n",
    "\n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73edcdc8-56a0-4088-b049-e5dfb71728fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Preview of processed dataset:\n",
      "          N         P         K  temperature  humidity        ph  rainfall  \\\n",
      "0  0.642857  0.289655  0.209756     0.345886  0.790267  0.466264  0.656458   \n",
      "1  0.607143  0.400000  0.200000     0.371445  0.770633  0.549480  0.741675   \n",
      "2  0.428571  0.379310  0.214634     0.406854  0.793977  0.674219  0.875710   \n",
      "3  0.528571  0.241379  0.195122     0.506901  0.768751  0.540508  0.799905   \n",
      "4  0.557143  0.289655  0.204878     0.324378  0.785626  0.641291  0.871231   \n",
      "\n",
      "   Temparature  Humidity  Moisture  ...        Summary Precip Type  \\\n",
      "0        0.512  0.010125     0.445  ...  Partly Cloudy        rain   \n",
      "1        0.512  0.010125     0.445  ...  Partly Cloudy        rain   \n",
      "2        0.512  0.010125     0.445  ...  Partly Cloudy        rain   \n",
      "3        0.512  0.010125     0.445  ...  Partly Cloudy        rain   \n",
      "4        0.512  0.010125     0.445  ...  Partly Cloudy        rain   \n",
      "\n",
      "  Temperature (C) Apparent Temperature (C) Wind Speed (km/h)  \\\n",
      "0        0.547925                 0.592246          0.156077   \n",
      "1        0.547925                 0.592246          0.156077   \n",
      "2        0.547925                 0.592246          0.156077   \n",
      "3        0.547925                 0.592246          0.156077   \n",
      "4        0.547925                 0.592246          0.156077   \n",
      "\n",
      "  Wind Bearing (degrees)  Visibility (km)  Loud Cover  Pressure (millibars)  \\\n",
      "0               0.501393            0.624         0.0              0.971397   \n",
      "1               0.501393            0.624         0.0              0.971397   \n",
      "2               0.501393            0.624         0.0              0.971397   \n",
      "3               0.501393            0.624         0.0              0.971397   \n",
      "4               0.501393            0.624         0.0              0.971397   \n",
      "\n",
      "                       Daily Summary  \n",
      "0  Mostly cloudy throughout the day.  \n",
      "1  Mostly cloudy throughout the day.  \n",
      "2  Mostly cloudy throughout the day.  \n",
      "3  Mostly cloudy throughout the day.  \n",
      "4  Mostly cloudy throughout the day.  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      " Features shape: (106653, 24), Labels shape: (106653,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Fill missing values with median (numeric) or mode (categorical).\"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in [\"float64\", \"int64\"]:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        else:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return df\n",
    "\n",
    "def scale_features(df):\n",
    "    \"\"\"Scale numeric features between 0 and 1.\"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    return df\n",
    "\n",
    "def load_and_prepare_data(path: str):\n",
    "    \"\"\"Load final dataset, clean it, scale features, and split X/y.\"\"\"\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "    # Handle missing + scaling\n",
    "    df = handle_missing_values(df)\n",
    "    df = scale_features(df)\n",
    "\n",
    "    # Ensure label column exists\n",
    "    if \"label\" not in df.columns:\n",
    "        raise KeyError(\" 'label' column not found in dataset! Make sure preprocessing includes target labels.\")\n",
    "\n",
    "    X = df.drop(\"label\", axis=1)\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Debug Run\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    processed_path = r\"C:\\Users\\tusha\\Downloads\\Sustainable-Crop-Recommendation\\data\\processed\\Crop_recommendation_clean.csv\"\n",
    "    \n",
    "    X, y = load_and_prepare_data(processed_path)\n",
    "\n",
    "    print(\"\\n Preview of processed dataset:\")\n",
    "    print(X.head())\n",
    "    print(f\"\\n Features shape: {X.shape}, Labels shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17178f3a-2c2c-405c-a1a7-c398cf16e795",
   "metadata": {},
   "source": [
    "# src/model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4c6adf8-1d19-45b9-a56a-b846228a8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d2228de-06aa-44f4-8950-0616068845f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Train a Single Model\n",
    "# -----------------------------\n",
    "def train_model(X_train, y_train, model_type=\"RandomForest\", random_state=42):\n",
    "    \"\"\"\n",
    "    Train a single model given training data.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame or np.ndarray): Training features\n",
    "        y_train (pd.Series or np.ndarray): Training labels\n",
    "        model_type (str): Model type (\"RandomForest\", \"SVM\", \"DecisionTree\")\n",
    "        random_state (int): Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        sklearn model: Fitted model\n",
    "    \"\"\"\n",
    "    if model_type == \"RandomForest\":\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    elif model_type == \"SVM\":\n",
    "        # Use moderate C to avoid long training on large datasets\n",
    "        model = SVC(kernel=\"rbf\", C=10, probability=True, random_state=random_state)\n",
    "    elif model_type == \"DecisionTree\":\n",
    "        model = DecisionTreeClassifier(random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(f\" Unsupported model_type: {model_type}\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd2f6282-a93f-4380-9261-a0132fdbcada",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Evaluate Model\n",
    "# -----------------------------\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on test data.\n",
    "\n",
    "    Args:\n",
    "        model: Trained sklearn model\n",
    "        X_test (pd.DataFrame): Test features\n",
    "        y_test (pd.Series): True labels\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"accuracy\": float, \"report\": str}\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return {\"accuracy\": acc, \"report\": report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9bae8-02ba-447d-9ffa-715482238ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File loaded successfully!\n",
      "Shape of dataset: (106653, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>ph</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>label</th>\n",
       "      <th>Temparature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>...</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Precip Type</th>\n",
       "      <th>Temperature (C)</th>\n",
       "      <th>Apparent Temperature (C)</th>\n",
       "      <th>Wind Speed (km/h)</th>\n",
       "      <th>Wind Bearing (degrees)</th>\n",
       "      <th>Visibility (km)</th>\n",
       "      <th>Loud Cover</th>\n",
       "      <th>Pressure (millibars)</th>\n",
       "      <th>Daily Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>20.879744</td>\n",
       "      <td>82.002744</td>\n",
       "      <td>6.502985</td>\n",
       "      <td>202.935536</td>\n",
       "      <td>rice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>21.770462</td>\n",
       "      <td>80.319644</td>\n",
       "      <td>7.038096</td>\n",
       "      <td>226.655537</td>\n",
       "      <td>rice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>23.004459</td>\n",
       "      <td>82.320763</td>\n",
       "      <td>7.840207</td>\n",
       "      <td>263.964248</td>\n",
       "      <td>rice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>26.491096</td>\n",
       "      <td>80.158363</td>\n",
       "      <td>6.980401</td>\n",
       "      <td>242.864034</td>\n",
       "      <td>rice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>20.130175</td>\n",
       "      <td>81.604873</td>\n",
       "      <td>7.628473</td>\n",
       "      <td>262.717340</td>\n",
       "      <td>rice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      N     P     K  temperature   humidity        ph    rainfall label  \\\n",
       "0  90.0  42.0  43.0    20.879744  82.002744  6.502985  202.935536  rice   \n",
       "1  85.0  58.0  41.0    21.770462  80.319644  7.038096  226.655537  rice   \n",
       "2  60.0  55.0  44.0    23.004459  82.320763  7.840207  263.964248  rice   \n",
       "3  74.0  35.0  40.0    26.491096  80.158363  6.980401  242.864034  rice   \n",
       "4  78.0  42.0  42.0    20.130175  81.604873  7.628473  262.717340  rice   \n",
       "\n",
       "   Temparature  Humidity  ...  Summary Precip Type Temperature (C)  \\\n",
       "0          NaN       NaN  ...      NaN         NaN             NaN   \n",
       "1          NaN       NaN  ...      NaN         NaN             NaN   \n",
       "2          NaN       NaN  ...      NaN         NaN             NaN   \n",
       "3          NaN       NaN  ...      NaN         NaN             NaN   \n",
       "4          NaN       NaN  ...      NaN         NaN             NaN   \n",
       "\n",
       "  Apparent Temperature (C) Wind Speed (km/h) Wind Bearing (degrees)  \\\n",
       "0                      NaN               NaN                    NaN   \n",
       "1                      NaN               NaN                    NaN   \n",
       "2                      NaN               NaN                    NaN   \n",
       "3                      NaN               NaN                    NaN   \n",
       "4                      NaN               NaN                    NaN   \n",
       "\n",
       "  Visibility (km)  Loud Cover  Pressure (millibars)  Daily Summary  \n",
       "0             NaN         NaN                   NaN            NaN  \n",
       "1             NaN         NaN                   NaN            NaN  \n",
       "2             NaN         NaN                   NaN            NaN  \n",
       "3             NaN         NaN                   NaN            NaN  \n",
       "4             NaN         NaN                   NaN            NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Encoders saved.\n",
      "üíæ Train/Test splits saved in C:\\Users\\tusha\\data/splits\n",
      "\n",
      "üìä Evaluation for RandomForest:\n",
      "‚úÖ Accuracy: 0.9999\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       apple       1.00      1.00      1.00     20911\n",
      "      banana       1.00      1.00      1.00        20\n",
      "   blackgram       1.00      1.00      1.00        20\n",
      "    chickpea       1.00      1.00      1.00        20\n",
      "     coconut       1.00      1.00      1.00        20\n",
      "      coffee       1.00      1.00      1.00        20\n",
      "      cotton       1.00      1.00      1.00        20\n",
      "      grapes       1.00      1.00      1.00        20\n",
      "        jute       1.00      0.95      0.97        20\n",
      " kidneybeans       1.00      1.00      1.00        20\n",
      "      lentil       1.00      0.95      0.97        20\n",
      "       maize       1.00      1.00      1.00        20\n",
      "       mango       1.00      1.00      1.00        20\n",
      "   mothbeans       0.95      1.00      0.98        20\n",
      "    mungbean       1.00      1.00      1.00        20\n",
      "   muskmelon       1.00      1.00      1.00        20\n",
      "      orange       1.00      1.00      1.00        20\n",
      "      papaya       1.00      1.00      1.00        20\n",
      "  pigeonpeas       1.00      1.00      1.00        20\n",
      " pomegranate       1.00      1.00      1.00        20\n",
      "        rice       0.95      1.00      0.98        20\n",
      "  watermelon       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00     21331\n",
      "   macro avg       1.00      1.00      1.00     21331\n",
      "weighted avg       1.00      1.00      1.00     21331\n",
      "\n",
      "Confusion Matrix:\n",
      " [[20911     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]\n",
      " [    0    20     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0    20     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0    20     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0    20     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0    20     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0    20     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0    20     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0    19     0     0     0\n",
      "      0     0     0     0     0     0     0     0     1     0]\n",
      " [    0     0     0     0     0     0     0     0     0    20     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0    19     0\n",
      "      0     1     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0    20\n",
      "      0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     20     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0    20     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0    20     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0    20     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0    20     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0    20     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0    20     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0    20     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0    20     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0    20]]\n",
      "üå≥ Random Forest Accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Full Pipeline Training (in Jupyter Notebook)\n",
    "# -----------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# ‚úÖ Use absolute path for Jupyter\n",
    "csv_path = r\"C:\\Users\\tusha\\Downloads\\Sustainable-Crop-Recommendation\\data\\processed\\Crop_recommendation_clean.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Train Model\n",
    "# -----------------------------\n",
    "def train_model(X_train, y_train, model_name):\n",
    "    if model_name == \"RandomForest\":\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif model_name == \"SVM\":\n",
    "        model = SVC(kernel=\"rbf\", probability=True, random_state=42)\n",
    "    elif model_name == \"DecisionTree\":\n",
    "        model = DecisionTreeClassifier(random_state=42)\n",
    "    else:\n",
    "        raise ValueError(f\"‚ùå Unknown model: {model_name}\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Evaluate Model\n",
    "# -----------------------------\n",
    "def evaluate_model(model, X_test, y_test, label_encoder, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\nüìä Evaluation for {model_name}:\")\n",
    "    print(f\"‚úÖ Accuracy: {acc:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"report\": report,\n",
    "        \"confusion_matrix\": cm\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Training Pipeline\n",
    "# -----------------------------\n",
    "def train_models(csv_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, low_memory=False)\n",
    "        print(\"‚úÖ File loaded successfully!\")\n",
    "        print(\"Shape of dataset:\", df.shape)\n",
    "        display(df.head())\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå File not found at:\", csv_path)\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error while loading file:\", e)\n",
    "        return {}\n",
    "\n",
    "    # -----------------------------\n",
    "    # Handle missing values\n",
    "    # -----------------------------\n",
    "    for col in df.select_dtypes(include=\"number\").columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "    # Separate features and target\n",
    "    if \"label\" not in df.columns:\n",
    "        raise KeyError(\"‚ùå CSV must contain a 'label' column as target.\")\n",
    "    X = df.drop(\"label\", axis=1)\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    # Encode categorical features\n",
    "    encoders = {}\n",
    "    for col in X.select_dtypes(include=[\"object\"]).columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "        encoders[col] = le\n",
    "\n",
    "    # Encode target\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    encoders[\"label\"] = label_encoder\n",
    "\n",
    "    # Scale numeric features\n",
    "    numeric_cols = X.select_dtypes(include=\"number\").columns\n",
    "    scaler = StandardScaler()\n",
    "    X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "\n",
    "    # Save encoders\n",
    "    models_dir = os.path.join(os.getcwd(), \"outputs/models\")\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    joblib.dump(encoders, os.path.join(models_dir, \"encoders.pkl\"))\n",
    "    print(\"üíæ Encoders saved.\")\n",
    "\n",
    "    # Stratified split\n",
    "    min_class_count = pd.Series(y).value_counts().min()\n",
    "    stratify = y if min_class_count >= 2 else None\n",
    "    if stratify is None:\n",
    "        print(\"‚ö†Ô∏è Some classes <2 samples. Disabling stratification.\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=stratify\n",
    "    )\n",
    "\n",
    "    # Save splits\n",
    "    splits_dir = os.path.join(os.getcwd(), \"data/splits\")\n",
    "    os.makedirs(splits_dir, exist_ok=True)\n",
    "    pd.concat([X_train, pd.Series(y_train, name=\"label\")], axis=1).to_csv(\n",
    "        os.path.join(splits_dir, \"train.csv\"), index=False\n",
    "    )\n",
    "    pd.concat([X_test, pd.Series(y_test, name=\"label\")], axis=1).to_csv(\n",
    "        os.path.join(splits_dir, \"test.csv\"), index=False\n",
    "    )\n",
    "    print(f\"üíæ Train/Test splits saved in {splits_dir}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # -----------------------------\n",
    "    # Train & Evaluate Multiple Models\n",
    "    # -----------------------------\n",
    "    rf = train_model(X_train, y_train, \"RandomForest\")\n",
    "    metrics_rf = evaluate_model(rf, X_test, y_test, label_encoder, \"RandomForest\")\n",
    "    results[\"RandomForest\"] = metrics_rf\n",
    "    joblib.dump(rf, os.path.join(models_dir, \"random_forest.pkl\"))\n",
    "    print(f\"üå≥ Random Forest Accuracy: {metrics_rf['accuracy']:.4f}\")\n",
    "\n",
    "    svm = train_model(X_train, y_train, \"SVM\")\n",
    "    metrics_svm = evaluate_model(svm, X_test, y_test, label_encoder, \"SVM\")\n",
    "    results[\"SVM\"] = metrics_svm\n",
    "    joblib.dump(svm, os.path.join(models_dir, \"svm_model.pkl\"))\n",
    "    print(f\"üìà SVM Accuracy: {metrics_svm['accuracy']:.4f}\")\n",
    "\n",
    "    dt = train_model(X_train, y_train, \"DecisionTree\")\n",
    "    metrics_dt = evaluate_model(dt, X_test, y_test, label_encoder, \"DecisionTree\")\n",
    "    results[\"DecisionTree\"] = metrics_dt\n",
    "    joblib.dump(dt, os.path.join(models_dir, \"decision_tree.pkl\"))\n",
    "    print(f\"üåø Decision Tree Accuracy: {metrics_dt['accuracy']:.4f}\")\n",
    "\n",
    "    # Save evaluation report\n",
    "    reports_dir = os.path.join(os.getcwd(), \"outputs/reports\")\n",
    "    os.makedirs(reports_dir, exist_ok=True)\n",
    "    report_path = os.path.join(reports_dir, \"accuracy_report.txt\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        for model, metrics in results.items():\n",
    "            f.write(f\"{model}: {metrics['accuracy']:.4f}\\n\")\n",
    "        f.write(\"\\nClassification Report (Random Forest):\\n\")\n",
    "        f.write(metrics_rf[\"report\"])\n",
    "    print(f\"üìä Accuracy report saved at {report_path}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# Run in Notebook\n",
    "# -----------------------------\n",
    "results = train_models(csv_path)\n",
    "print(\"\\n‚úÖ Training complete. Results:\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}: {metrics['accuracy']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c9a393-cc6d-409c-bc4b-047cc65dc822",
   "metadata": {},
   "source": [
    "## cro_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40564d50-2bb9-4ff5-94f3-c8e326628164",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Global variable to hold models (optional, keeps memory efficient)\n",
    "MODELS = None\n",
    "\n",
    "def load_models(models_path: str = r\"C:\\Users\\tusha\\Downloads\\Sustainable-Crop-Recommendation\\outputs\\models\"):\n",
    "    \"\"\"\n",
    "    Load trained models and encoders.\n",
    "    Returns a dict with Random Forest, SVM, and encoders.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"random_forest\": joblib.load(os.path.join(models_path, \"random_forest.pkl\")),\n",
    "        \"svm\": joblib.load(os.path.join(models_path, \"svm_model.pkl\")),\n",
    "        \"encoders\": joblib.load(os.path.join(models_path, \"encoders.pkl\")),\n",
    "    }\n",
    "    return models\n",
    "\n",
    "def preprocess_input(user_input: dict, encoders: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert user input dict to DataFrame and encode categorical features\n",
    "    \"\"\"\n",
    "    # Convert keys to lowercase to match training feature names\n",
    "    user_input = {k.lower(): v for k, v in user_input.items()}\n",
    "    df = pd.DataFrame([user_input])\n",
    "\n",
    "    # Encode categorical columns using saved encoders\n",
    "    for col, le in encoders.items():\n",
    "        if col != \"label\" and col in df.columns:\n",
    "            df[col] = le.transform(df[col])\n",
    "    return df\n",
    "\n",
    "def predict_crop(user_input: dict, models: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Predict crop recommendation using all trained models (RandomForest, SVM).\n",
    "    \n",
    "    Args:\n",
    "        user_input (dict): Dictionary of farm features (N, P, K, temperature, humidity, ph, rainfall)\n",
    "        models (dict): Dictionary containing trained models and encoders\n",
    "\n",
    "    Returns:\n",
    "        dict: { \"random_forest\": \"rice\", \"svm\": \"wheat\" }\n",
    "    \"\"\"\n",
    "    # Preprocess input using saved encoders\n",
    "    X = preprocess_input(user_input, models[\"encoders\"])\n",
    "\n",
    "    predictions = {}\n",
    "    for model_name, model in models.items():\n",
    "        if model_name == \"encoders\":\n",
    "            continue  # skip encoders, not a model\n",
    "        pred_index = model.predict(X)[0]\n",
    "        crop = models[\"encoders\"][\"label\"].inverse_transform([pred_index])[0]\n",
    "        predictions[model_name] = crop\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load models once\n",
    "    models = load_models()\n",
    "\n",
    "    # Example input\n",
    "    sample_input = {\n",
    "        \"n\": 90,\n",
    "        \"p\": 42,\n",
    "        \"k\": 43,\n",
    "        \"temperature\": 20.8,\n",
    "        \"humidity\": 82,\n",
    "        \"ph\": 6.5,\n",
    "        \"rainfall\": 200,\n",
    "    }\n",
    "\n",
    "    results = predict_crop(sample_input, models)\n",
    "    for model_name, crop in results.items():\n",
    "        print(f\"üå± Recommended Crop ({model_name}): {crop}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a048d3-6b6a-4bb3-9fb2-c0f0216bd88f",
   "metadata": {},
   "source": [
    "## feature_enginerring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db1021-01e0-4c4b-9d33-8703e84730e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Feature engineering utilities.\n",
    "Adds extra sustainability-focused features like soil fertility index or drought score.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def add_soil_fertility_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a soil fertility index = weighted sum of N, P, K.\n",
    "    \"\"\"\n",
    "    df[\"soil_fertility_index\"] = (\n",
    "        0.4 * df[\"n\"] + 0.3 * df[\"p\"] + 0.3 * df[\"k\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_drought_score(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add drought score based on rainfall & temperature.\n",
    "    Higher temperature & lower rainfall ‚Üí higher drought score.\n",
    "    \"\"\"\n",
    "    df[\"drought_score\"] = (\n",
    "        (df[\"temperature\"] / df[\"temperature\"].max()) * 0.6\n",
    "        + (1 - df[\"rainfall\"] / df[\"rainfall\"].max()) * 0.4\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply all feature engineering functions.\n",
    "    \"\"\"\n",
    "    df = add_soil_fertility_index(df)\n",
    "    df = add_drought_score(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518b097-b077-41e0-9de7-9fd665082cd9",
   "metadata": {},
   "source": [
    "## model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b3ebc-1ddc-4014-b26c-9069fd743e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model evaluation utilities.\n",
    "Generate reports, confusion matrices, classification reports, and save them.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, label_encoder, model_name: str):\n",
    "    \"\"\"\n",
    "    Evaluate model performance and save confusion matrix + report.\n",
    "    \"\"\"\n",
    "    outputs_dir = os.path.join(\"outputs\", \"reports\")\n",
    "    os.makedirs(outputs_dir, exist_ok=True)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Save classification report\n",
    "    report_path = os.path.join(outputs_dir, f\"{model_name}_report.txt\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(f\"Accuracy: {acc:.4f}\\n\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(\n",
    "            classification_report(\n",
    "                y_test,\n",
    "                y_pred,\n",
    "                target_names=label_encoder.classes_,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Save confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=label_encoder.classes_,\n",
    "        yticklabels=label_encoder.classes_,\n",
    "    )\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "\n",
    "    cm_path = os.path.join(outputs_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\" {model_name} evaluated. Accuracy: {acc:.4f}\")\n",
    "    print(f\" Reports saved to {outputs_dir}\")\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a1255b-ad4d-414c-ae0f-e2b7bd5b23b9",
   "metadata": {},
   "source": [
    "## visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a268a-1ff6-4ddf-8e08-10a4e1651b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualization utilities.\n",
    "Generate correlation heatmaps and feature importance plots.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(df: pd.DataFrame, save_path=\"outputs/reports/correlation_heatmap.png\"):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Heatmap of Features\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"üìä Correlation heatmap saved at {save_path}\")\n",
    "\n",
    "\n",
    "def plot_feature_importance(model, feature_names, save_path=\"outputs/reports/feature_importance.png\"):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    importances = model.feature_importances_\n",
    "    sorted_idx = importances.argsort()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(sorted_idx)), importances[sorted_idx], align=\"center\")\n",
    "    plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"üåü Feature importance plot saved at {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb394e-26d8-47f9-933e-f6ab3383ac1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
