{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad75dbc",
   "metadata": {},
   "source": [
    "# üå± Sustainable Crop Recommendation System\n",
    "**Author:** Tushar Kapoor  \n",
    "**Project:** Predict the best crop for a region based on soil, climate, and season.  \n",
    "**Purpose:** Final submission-ready notebook demonstrating end-to-end workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c53b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = \"../data\"\n",
    "RAW_DIR = os.path.join(BASE_DIR, \"raw\")\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, \"processed\")\n",
    "\n",
    "# Raw files\n",
    "crop_file = os.path.join(RAW_DIR, \"crop_recommendation.csv\")\n",
    "weather_file = os.path.join(RAW_DIR, \"weatherHistory.csv\")\n",
    "core_file = os.path.join(RAW_DIR, \"data_core.csv\")  # adjust if name is different\n",
    "\n",
    "# Processed file (optional: save combined cleaned data)\n",
    "PROCESSED_PATH = os.path.join(PROCESSED_DIR, \"cleaned_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f85227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_DIR = \"../data\"\n",
    "RAW_DIR = os.path.join(BASE_DIR, \"raw\")\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, \"processed\")\n",
    "\n",
    "# Raw files\n",
    "crop_file = os.path.join(RAW_DIR, \"crop_recommendation.csv\")\n",
    "weather_file = os.path.join(RAW_DIR, \"weatherHistory.csv\")\n",
    "core_file = os.path.join(RAW_DIR, \"data_core.csv\")  # adjust if name is different\n",
    "\n",
    "# Processed file (optional: save combined cleaned data)\n",
    "PROCESSED_PATH = os.path.join(PROCESSED_DIR, \"cleaned_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e8dbe83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crop_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example: merge datasets on a common column if exists\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Here, we‚Äôll assume 'date' or 'location' might exist\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Adjust merge keys according to actual datasets\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m crop_df\u001b[38;5;241m.\u001b[39mcopy()  \u001b[38;5;66;03m# start with crop dataset\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Merge with core data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m core_df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'crop_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Example: merge datasets on a common column if exists\n",
    "# Here, we‚Äôll assume 'date' or 'location' might exist\n",
    "# Adjust merge keys according to actual datasets\n",
    "combined_df = crop_df.copy()  # start with crop dataset\n",
    "\n",
    "# Merge with core data\n",
    "if 'location' in core_df.columns:\n",
    "    combined_df = combined_df.merge(core_df, on='location', how='left')\n",
    "\n",
    "# Merge with weather data\n",
    "if 'date' in weather_df.columns and 'date' in combined_df.columns:\n",
    "    combined_df = combined_df.merge(weather_df, on='date', how='left')\n",
    "\n",
    "# Quick info\n",
    "print(combined_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31aa0cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop dataset shape: (2200, 8)\n",
      "Weather dataset shape: (96453, 12)\n",
      "Core dataset shape: (8000, 9)\n",
      "Combined dataset shape: (219358, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tusha\\AppData\\Local\\Temp\\ipykernel_22176\\1843846656.py:64: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  combined_df.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\tusha\\AppData\\Local\\Temp\\ipykernel_22176\\1843846656.py:64: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\tusha\\AppData\\Local\\Temp\\ipykernel_22176\\1843846656.py:65: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  combined_df.fillna(method='bfill', inplace=True)\n",
      "c:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 101\u001b[0m\n\u001b[0;32m     98\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 101\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    102\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m    103\u001b[0m     acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    364\u001b[0m     X,\n\u001b[0;32m    365\u001b[0m     y,\n\u001b[0;32m    366\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    367\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    368\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE,\n\u001b[0;32m    369\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    370\u001b[0m )\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[1;32mc:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1279\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     )\n\u001b[0;32m   1263\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1264\u001b[0m     X,\n\u001b[0;32m   1265\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1277\u001b[0m )\n\u001b[1;32m-> 1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1281\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1289\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[1;32m-> 1289\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1290\u001b[0m         y,\n\u001b[0;32m   1291\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1292\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1293\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1294\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1295\u001b[0m         input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1296\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1297\u001b[0m     )\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1299\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
      "File \u001b[1;32mc:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1046\u001b[0m     )\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1049\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1050\u001b[0m         array,\n\u001b[0;32m   1051\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1052\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1053\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1054\u001b[0m     )\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    127\u001b[0m     X,\n\u001b[0;32m    128\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    129\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    130\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    131\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    132\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    133\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 04_final_project.ipynb\n",
    "# World-class, ready-to-run notebook\n",
    "# ===============================\n",
    "\n",
    "# 1Ô∏è‚É£ Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------------\n",
    "# 2Ô∏è‚É£ Define Paths\n",
    "BASE_DIR = \"../data\"\n",
    "RAW_DIR = os.path.join(BASE_DIR, \"raw\")\n",
    "MODEL_DIR = \"../outputs/models\"\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "crop_file = os.path.join(RAW_DIR, \"crop_recommendation.csv\")\n",
    "weather_file = os.path.join(RAW_DIR, \"weatherHistory.csv\")\n",
    "core_file = os.path.join(RAW_DIR, \"data_core.csv\")\n",
    "\n",
    "# ===============================\n",
    "# 3Ô∏è‚É£ Load Datasets\n",
    "crop_df = pd.read_csv(crop_file)\n",
    "weather_df = pd.read_csv(weather_file)\n",
    "core_df = pd.read_csv(core_file)\n",
    "\n",
    "print(\"Crop dataset shape:\", crop_df.shape)\n",
    "print(\"Weather dataset shape:\", weather_df.shape)\n",
    "print(\"Core dataset shape:\", core_df.shape)\n",
    "\n",
    "# ===============================\n",
    "# 4Ô∏è‚É£ Merge/Combine Datasets\n",
    "# Start with crop dataset\n",
    "combined_df = crop_df.copy()\n",
    "\n",
    "# Merge with core_df if common column exists\n",
    "common_cols_core = set(combined_df.columns).intersection(core_df.columns)\n",
    "if common_cols_core:\n",
    "    merge_col = list(common_cols_core)[0]  # pick first common column\n",
    "    combined_df = combined_df.merge(core_df, on=merge_col, how='left')\n",
    "\n",
    "# Merge with weather_df if common column exists\n",
    "common_cols_weather = set(combined_df.columns).intersection(weather_df.columns)\n",
    "if common_cols_weather:\n",
    "    merge_col = list(common_cols_weather)[0]\n",
    "    combined_df = combined_df.merge(weather_df, on=merge_col, how='left')\n",
    "\n",
    "print(\"Combined dataset shape:\", combined_df.shape)\n",
    "\n",
    "# ===============================\n",
    "# 5Ô∏è‚É£ Preprocessing\n",
    "# Handle missing values\n",
    "combined_df.fillna(combined_df.median(numeric_only=True), inplace=True)\n",
    "combined_df.fillna(method='ffill', inplace=True)\n",
    "combined_df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in combined_df.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    combined_df[col] = le.fit_transform(combined_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Features & Target\n",
    "# Assume last column is the target (adjust if needed)\n",
    "X = combined_df.iloc[:, :-1]\n",
    "y = combined_df.iloc[:, -1]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 6Ô∏è‚É£ Train Multiple Models\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"SVM\": SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "best_model_name = None\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_model_name = name\n",
    "        best_model = model\n",
    "\n",
    "print(f\"\\n‚úÖ Best Model: {best_model_name} with Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 7Ô∏è‚É£ Save Best Model\n",
    "model_path = os.path.join(MODEL_DIR, f\"{best_model_name.lower()}_model.pkl\")\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Saved best model at: {model_path}\")\n",
    "\n",
    "# ===============================\n",
    "# 8Ô∏è‚É£ Evaluate Model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f\"{best_model_name} Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ===============================\n",
    "# 9Ô∏è‚É£ Summary\n",
    "print(f\"\\nModel Training Completed. Best Model: {best_model_name}\")\n",
    "print(\"All preprocessing, training, evaluation done successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786e793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tusha\\AppData\\Local\\Temp\\ipykernel_22176\\2836527532.py:33: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined_df[col].fillna(combined_df[col].median(), inplace=True)\n",
      "C:\\Users\\tusha\\AppData\\Local\\Temp\\ipykernel_22176\\2836527532.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined_df[col].fillna(combined_df[col].mode()[0], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RandomForest ---\n",
      "Accuracy: 0.6106992898242704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tusha\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89         5\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      0.60      0.75         5\n",
      "           3       0.75      0.60      0.67         5\n",
      "           4       0.80      0.80      0.80         5\n",
      "           5       0.75      0.60      0.67         5\n",
      "           6       0.40      0.40      0.40         5\n",
      "           7       0.43      0.60      0.50         5\n",
      "           8       0.86      0.43      0.57        14\n",
      "           9       1.00      0.33      0.50         9\n",
      "          10       1.00      0.20      0.33         5\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       1.00      1.00      1.00         5\n",
      "          13       1.00      0.80      0.89         5\n",
      "          14       1.00      0.40      0.57         5\n",
      "          15       1.00      0.20      0.33         5\n",
      "          16       0.75      0.60      0.67         5\n",
      "          17       0.60      0.60      0.60         5\n",
      "          18       0.80      0.80      0.80         5\n",
      "          19       1.00      0.60      0.75         5\n",
      "          20       0.80      0.80      0.80         5\n",
      "          21       1.00      0.80      0.89         5\n",
      "          22       0.67      0.80      0.73         5\n",
      "          23       1.00      0.40      0.57         5\n",
      "          24       0.67      0.80      0.73         5\n",
      "          25       0.56      0.56      0.56         9\n",
      "          26       0.80      0.80      0.80         5\n",
      "          27       1.00      0.40      0.57         5\n",
      "          28       1.00      0.67      0.80         9\n",
      "          29       0.69      0.72      0.70       183\n",
      "          30       1.00      0.60      0.75         5\n",
      "          31       1.00      0.40      0.57         5\n",
      "          32       0.91      0.42      0.57        24\n",
      "          33       0.89      0.65      0.75        48\n",
      "          34       1.00      0.60      0.75         5\n",
      "          35       0.68      0.58      0.62       840\n",
      "          36       0.71      1.00      0.83         5\n",
      "          37       0.76      0.47      0.58       255\n",
      "          38       1.00      0.80      0.89         5\n",
      "          39       0.80      0.67      0.73        58\n",
      "          40       1.00      0.59      0.74        29\n",
      "          41       0.83      0.71      0.77        62\n",
      "          42       0.86      0.25      0.39        24\n",
      "          43       0.79      0.50      0.62       293\n",
      "          44       0.50      0.40      0.44         5\n",
      "          45       0.88      0.60      0.71        77\n",
      "          46       1.00      0.60      0.75         5\n",
      "          47       0.88      0.87      0.88        53\n",
      "          48       0.78      0.62      0.69        58\n",
      "          49       0.88      0.62      0.73        24\n",
      "          50       1.00      0.60      0.75         5\n",
      "          51       0.77      0.63      0.69        86\n",
      "          52       0.82      0.79      0.80        62\n",
      "          53       1.00      0.50      0.67         4\n",
      "          54       0.00      0.00      0.00         5\n",
      "          55       0.67      0.22      0.33         9\n",
      "          56       0.90      1.00      0.95         9\n",
      "          57       0.66      0.59      0.62       715\n",
      "          58       0.85      0.67      0.75        33\n",
      "          59       0.95      0.83      0.89        24\n",
      "          60       0.84      0.92      0.88       173\n",
      "          61       0.80      0.57      0.67        82\n",
      "          62       0.76      0.79      0.77        43\n",
      "          63       0.71      0.67      0.69       538\n",
      "          64       0.78      0.72      0.75       106\n",
      "          65       1.00      0.60      0.75         5\n",
      "          66       1.00      0.20      0.33         5\n",
      "          67       1.00      0.40      0.57         5\n",
      "          68       0.83      1.00      0.91         5\n",
      "          69       0.50      0.20      0.29         5\n",
      "          70       0.83      1.00      0.91         5\n",
      "          71       0.50      0.11      0.18         9\n",
      "          72       0.83      0.53      0.65        19\n",
      "          73       1.00      0.40      0.57         5\n",
      "          74       0.50      0.22      0.31         9\n",
      "          75       1.00      0.40      0.57         5\n",
      "          76       1.00      0.80      0.89         5\n",
      "          77       1.00      0.33      0.50         9\n",
      "          78       1.00      0.20      0.33         5\n",
      "          79       1.00      0.80      0.89         5\n",
      "          80       0.67      0.29      0.40        14\n",
      "          81       1.00      0.60      0.75         5\n",
      "          82       0.62      0.39      0.48        38\n",
      "          83       0.78      0.43      0.55       317\n",
      "          84       0.83      0.56      0.67         9\n",
      "          85       0.62      0.56      0.59         9\n",
      "          86       0.75      0.60      0.67         5\n",
      "          87       0.00      0.00      0.00         5\n",
      "          88       0.57      0.29      0.38        14\n",
      "          89       0.80      0.86      0.83        14\n",
      "          90       0.75      0.60      0.67         5\n",
      "          91       0.86      0.56      0.68        43\n",
      "          92       0.67      0.22      0.33         9\n",
      "          93       1.00      0.40      0.57         5\n",
      "          94       0.83      0.41      0.55        86\n",
      "          95       0.78      0.30      0.43       254\n",
      "          96       0.86      0.44      0.58        43\n",
      "          97       1.00      0.29      0.44        14\n",
      "          98       0.82      0.64      0.72        72\n",
      "          99       0.64      0.37      0.47        19\n",
      "         100       0.73      0.33      0.46        24\n",
      "         101       0.83      0.56      0.67         9\n",
      "         102       0.80      0.80      0.80         5\n",
      "         103       0.80      0.33      0.47        24\n",
      "         104       0.74      0.56      0.64        57\n",
      "         105       0.80      0.67      0.73        24\n",
      "         106       0.90      1.00      0.95         9\n",
      "         107       1.00      0.80      0.89         5\n",
      "         108       0.75      0.62      0.68        29\n",
      "         109       1.00      0.60      0.75         5\n",
      "         110       0.60      0.60      0.60         5\n",
      "         111       0.58      0.90      0.70      4017\n",
      "         112       0.81      0.52      0.63        33\n",
      "         113       1.00      0.60      0.75         5\n",
      "         114       1.00      0.40      0.57         5\n",
      "         115       0.55      0.50      0.52        24\n",
      "         116       1.00      0.40      0.57         5\n",
      "         117       0.33      0.20      0.25         5\n",
      "         118       0.81      0.42      0.55       211\n",
      "         119       0.88      0.72      0.79        29\n",
      "         120       0.58      0.50      0.54        14\n",
      "         121       1.00      0.44      0.62         9\n",
      "         122       1.00      0.20      0.33         5\n",
      "         123       0.00      0.00      0.00         5\n",
      "         124       0.83      1.00      0.91         5\n",
      "         125       0.80      0.80      0.80         5\n",
      "         126       0.00      0.00      0.00         5\n",
      "         127       1.00      0.11      0.20         9\n",
      "         128       1.00      0.40      0.57         5\n",
      "         129       0.60      0.60      0.60         5\n",
      "         130       0.75      0.60      0.67         5\n",
      "         131       0.00      0.00      0.00         5\n",
      "         132       0.75      0.44      0.56       619\n",
      "         133       1.00      0.80      0.89         5\n",
      "         134       1.00      0.60      0.75         5\n",
      "         135       1.00      0.40      0.57         5\n",
      "         136       0.90      1.00      0.95         9\n",
      "         137       0.83      0.71      0.77        14\n",
      "         138       1.00      0.20      0.33         5\n",
      "         139       1.00      0.33      0.50         9\n",
      "         140       1.00      0.20      0.33         5\n",
      "         141       0.80      0.80      0.80         5\n",
      "         142       1.00      0.80      0.89         5\n",
      "         143       0.82      0.74      0.78       590\n",
      "         144       0.71      0.63      0.67        19\n",
      "         145       0.57      0.80      0.67         5\n",
      "         146       0.83      1.00      0.91         5\n",
      "         147       0.79      0.67      0.72        33\n",
      "         148       0.50      0.32      0.39        53\n",
      "         149       0.67      0.47      0.55        43\n",
      "         150       0.58      0.39      0.46        96\n",
      "         151       0.67      0.80      0.73         5\n",
      "         152       0.60      0.24      0.34       173\n",
      "         153       0.75      0.60      0.67         5\n",
      "         154       0.54      0.59      0.57       264\n",
      "         155       0.62      0.36      0.46        77\n",
      "         156       0.60      0.47      0.53       269\n",
      "         157       0.83      0.56      0.67         9\n",
      "         158       0.86      0.63      0.73        19\n",
      "         159       1.00      0.20      0.33         5\n",
      "         160       0.00      0.00      0.00         5\n",
      "         161       0.50      0.60      0.55         5\n",
      "         162       0.82      0.45      0.58        62\n",
      "         163       1.00      0.20      0.33         5\n",
      "         164       0.50      0.20      0.29         5\n",
      "         165       1.00      0.78      0.88         9\n",
      "         166       0.51      0.52      0.51       561\n",
      "         167       0.50      0.40      0.44         5\n",
      "         168       1.00      0.40      0.57         5\n",
      "         169       0.53      0.36      0.43       427\n",
      "         170       0.51      0.51      0.51      1037\n",
      "         171       0.00      0.00      0.00         5\n",
      "         172       1.00      0.20      0.33         5\n",
      "         173       0.20      0.20      0.20         5\n",
      "         174       1.00      0.40      0.57         5\n",
      "         175       0.67      0.37      0.47        38\n",
      "         176       1.00      1.00      1.00         5\n",
      "         177       0.75      0.60      0.67         5\n",
      "         178       0.75      0.86      0.80        14\n",
      "         179       1.00      0.40      0.57         5\n",
      "         180       0.80      0.80      0.80         5\n",
      "         181       0.65      0.42      0.51       144\n",
      "         182       0.50      0.40      0.44         5\n",
      "         183       0.80      0.45      0.58        53\n",
      "         184       0.50      0.20      0.29         5\n",
      "         185       1.00      0.20      0.33         5\n",
      "         186       0.67      0.40      0.50         5\n",
      "         187       0.67      0.30      0.41       178\n",
      "         188       0.70      0.22      0.33       288\n",
      "         189       1.00      0.60      0.75         5\n",
      "         190       0.40      0.11      0.17        19\n",
      "         191       0.75      0.33      0.46         9\n",
      "         192       0.60      0.21      0.32        14\n",
      "         193       0.78      0.42      0.55        43\n",
      "         194       1.00      1.00      1.00         5\n",
      "         195       1.00      0.60      0.75         5\n",
      "         196       1.00      0.20      0.33         5\n",
      "         197       0.51      0.67      0.58      1996\n",
      "         198       0.68      0.39      0.50        33\n",
      "         199       1.00      0.20      0.33         5\n",
      "         200       0.43      0.16      0.23        19\n",
      "         201       0.50      0.20      0.29         5\n",
      "         202       0.77      0.53      0.62        19\n",
      "         203       1.00      0.44      0.62         9\n",
      "         204       0.57      0.38      0.46       658\n",
      "         205       0.75      0.47      0.58        77\n",
      "         206       1.00      0.60      0.75         5\n",
      "         207       0.80      0.57      0.67        14\n",
      "         208       0.65      0.54      0.59        24\n",
      "         209       0.48      0.51      0.50      1234\n",
      "         210       1.00      1.00      1.00         5\n",
      "         211       1.00      1.00      1.00         5\n",
      "         212       1.00      0.60      0.75         5\n",
      "         213       0.33      0.20      0.25         5\n",
      "\n",
      "    accuracy                           0.61     19291\n",
      "   macro avg       0.78      0.52      0.60     19291\n",
      "weighted avg       0.63      0.61      0.60     19291\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4 0 0 ... 0 0 0]\n",
      " [0 5 0 ... 0 0 0]\n",
      " [0 0 3 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 5 0 0]\n",
      " [0 0 0 ... 0 3 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Sustainable Crop Recommendation - Production-Ready Notebook\n",
    "# ===========================\n",
    "\n",
    "# 1Ô∏è‚É£ Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# 2Ô∏è‚É£ Paths\n",
    "RAW_PATH = r\"C:\\Users\\tusha\\Downloads\\Sustainable-Crop-Recommendation\\data\\raw\"\n",
    "OUTPUT_PATH = r\"C:\\Users\\tusha\\Downloads\\Sustainable-Crop-Recommendation\\outputs\\models\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# 3Ô∏è‚É£ Load all CSV files\n",
    "files = [f for f in os.listdir(RAW_PATH) if f.endswith(\".csv\")]\n",
    "dfs = [pd.read_csv(os.path.join(RAW_PATH, f)) for f in files]\n",
    "\n",
    "# Combine datasets (adjust keys if needed)\n",
    "combined_df = pd.concat(dfs, axis=1)\n",
    "combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n",
    "\n",
    "# 4Ô∏è‚É£ Preprocessing\n",
    "# Handle missing values\n",
    "for col in combined_df.columns:\n",
    "    if combined_df[col].dtype in ['int64', 'float64']:\n",
    "        combined_df[col].fillna(combined_df[col].median(), inplace=True)\n",
    "    else:\n",
    "        combined_df[col].fillna(combined_df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Drop rows with missing target (assume last column is target)\n",
    "target_col = combined_df.columns[-1]\n",
    "combined_df = combined_df.dropna(subset=[target_col])\n",
    "\n",
    "# Encode categorical features\n",
    "cat_cols = combined_df.select_dtypes(include='object').columns.tolist()\n",
    "if target_col in cat_cols:\n",
    "    cat_cols.remove(target_col)\n",
    "\n",
    "for col in cat_cols:\n",
    "    combined_df[col] = LabelEncoder().fit_transform(combined_df[col])\n",
    "\n",
    "# Encode target if categorical\n",
    "y = combined_df[target_col]\n",
    "if y.dtype == 'object':\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Features\n",
    "X = combined_df.drop(columns=[target_col])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 5Ô∏è‚É£ Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 6Ô∏è‚É£ Model Training & Evaluation\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"SVM\": SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "best_acc = 0\n",
    "best_model_name = None\n",
    "best_model = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model_name = name\n",
    "        best_model = model\n",
    "\n",
    "print(f\"‚úÖ Best Model: {best_model_name} with accuracy {best_acc:.4f}\")\n",
    "\n",
    "# Save best model and scaler\n",
    "joblib.dump(best_model, os.path.join(OUTPUT_PATH, \"best_model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(OUTPUT_PATH, \"scaler.pkl\"))\n",
    "\n",
    "# 7Ô∏è‚É£ Production Prediction Function\n",
    "def predict_new_data(new_data_df):\n",
    "    \"\"\"\n",
    "    new_data_df: pandas DataFrame with the same features as training data (excluding target)\n",
    "    returns: predictions as list\n",
    "    \"\"\"\n",
    "    # Fill missing values same way as training\n",
    "    for col in new_data_df.columns:\n",
    "        if new_data_df[col].dtype in ['int64', 'float64']:\n",
    "            new_data_df[col].fillna(combined_df[col].median(), inplace=True)\n",
    "        else:\n",
    "            new_data_df[col].fillna(combined_df[col].mode()[0], inplace=True)\n",
    "    \n",
    "    # Encode categorical features same way\n",
    "    for col in new_data_df.select_dtypes(include='object').columns:\n",
    "        if col in cat_cols:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(combined_df[col])\n",
    "            new_data_df[col] = le.transform(new_data_df[col])\n",
    "    \n",
    "    # Scale\n",
    "    new_scaled = scaler.transform(new_data_df)\n",
    "    \n",
    "    # Predict\n",
    "    preds = best_model.predict(new_scaled)\n",
    "    return preds\n",
    "\n",
    "# 8Ô∏è‚É£ Optional: Feature Importance for Random Forest\n",
    "if best_model_name == \"RandomForest\":\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    feat_imp = pd.Series(best_model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=feat_imp.values, y=feat_imp.index)\n",
    "    plt.title(\"Feature Importance - Random Forest\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bda0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Sustainable Crop Recommendation - Robust Notebook\n",
    "# ===========================\n",
    "\n",
    "# 1Ô∏è‚É£ Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 2Ô∏è‚É£ Paths\n",
    "RAW_PATH = r\"C:\\Users\\tusha\\Downloads\\Sustainable-Crop-Recommendation\\data\\raw\"\n",
    "OUTPUT_PATH = r\"C:\\Users\\tusha\\Downloads\\Sustainable-Crop-Recommendation\\outputs\\models\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# 3Ô∏è‚É£ Load all CSV files safely\n",
    "files = [f for f in os.listdir(RAW_PATH) if f.endswith(\".csv\")]\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No CSV files found in raw folder!\")\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(RAW_PATH, f))\n",
    "        dfs.append(df)\n",
    "        print(f\"‚úÖ Loaded {f} with shape {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load {f}: {e}\")\n",
    "\n",
    "# Merge datasets safely\n",
    "try:\n",
    "    combined_df = pd.concat(dfs, axis=1)\n",
    "    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n",
    "    print(f\"‚úÖ Combined dataframe shape: {combined_df.shape}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error combining datasets: {e}\")\n",
    "\n",
    "# 4Ô∏è‚É£ Preprocessing with error handling\n",
    "try:\n",
    "    # Handle missing values\n",
    "    for col in combined_df.columns:\n",
    "        if combined_df[col].dtype in ['int64', 'float64']:\n",
    "            combined_df[col].fillna(combined_df[col].median(), inplace=True)\n",
    "        else:\n",
    "            combined_df[col].fillna(combined_df[col].mode()[0], inplace=True)\n",
    "\n",
    "    # Detect target column automatically (last column assumed)\n",
    "    target_col = combined_df.columns[-1]\n",
    "    if combined_df[target_col].isnull().all():\n",
    "        raise ValueError(\"Target column is completely empty!\")\n",
    "\n",
    "    y = combined_df[target_col]\n",
    "    if y.dtype == 'object':\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # Features\n",
    "    X = combined_df.drop(columns=[target_col])\n",
    "\n",
    "    # Encode categorical features\n",
    "    cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "    for col in cat_cols:\n",
    "        try:\n",
    "            X[col] = LabelEncoder().fit_transform(X[col])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not encode {col}: {e}\")\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    print(\"‚úÖ Preprocessing complete\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Preprocessing error: {e}\")\n",
    "\n",
    "# 5Ô∏è‚É£ Train-Test Split\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error in train-test split: {e}\")\n",
    "\n",
    "# 6Ô∏è‚É£ Model Training & Evaluation\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"SVM\": SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "best_acc = 0\n",
    "best_model_name = None\n",
    "best_model = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"--- {name} ---\")\n",
    "        print(\"Accuracy:\", acc)\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model_name = name\n",
    "            best_model = model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error training {name}: {e}\")\n",
    "\n",
    "if best_model is None:\n",
    "    raise RuntimeError(\"No model trained successfully!\")\n",
    "print(f\"‚úÖ Best Model: {best_model_name} with accuracy {best_acc:.4f}\")\n",
    "\n",
    "# Save model and scaler safely\n",
    "try:\n",
    "    joblib.dump(best_model, os.path.join(OUTPUT_PATH, \"best_model.pkl\"))\n",
    "    joblib.dump(scaler, os.path.join(OUTPUT_PATH, \"scaler.pkl\"))\n",
    "    print(\"‚úÖ Model and scaler saved\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save model/scaler: {e}\")\n",
    "\n",
    "# 7Ô∏è‚É£ Production Prediction Function with error handling\n",
    "def predict_new_data(new_data_df):\n",
    "    try:\n",
    "        # Fill missing values same as training\n",
    "        for col in new_data_df.columns:\n",
    "            if new_data_df[col].dtype in ['int64', 'float64']:\n",
    "                if col in combined_df.columns:\n",
    "                    new_data_df[col].fillna(combined_df[col].median(), inplace=True)\n",
    "                else:\n",
    "                    new_data_df[col].fillna(new_data_df[col].median(), inplace=True)\n",
    "            else:\n",
    "                if col in combined_df.columns:\n",
    "                    new_data_df[col].fillna(combined_df[col].mode()[0], inplace=True)\n",
    "                else:\n",
    "                    new_data_df[col].fillna(new_data_df[col].mode()[0], inplace=True)\n",
    "\n",
    "        # Encode categorical\n",
    "        for col in new_data_df.select_dtypes(include='object').columns:\n",
    "            if col in cat_cols:\n",
    "                le = LabelEncoder()\n",
    "                le.fit(combined_df[col])\n",
    "                new_data_df[col] = le.transform(new_data_df[col])\n",
    "            else:\n",
    "                new_data_df[col] = new_data_df[col].astype('category').cat.codes\n",
    "\n",
    "        # Scale\n",
    "        new_scaled = scaler.transform(new_data_df)\n",
    "\n",
    "        # Predict\n",
    "        preds = best_model.predict(new_scaled)\n",
    "        return preds\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Prediction error: {e}\")\n",
    "        return None\n",
    "\n",
    "# 8Ô∏è‚É£ Optional Feature Importance for Random Forest\n",
    "try:\n",
    "    if best_model_name == \"RandomForest\":\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "\n",
    "        feat_imp = pd.Series(best_model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(x=feat_imp.values, y=feat_imp.index)\n",
    "        plt.title(\"Feature Importance - Random Forest\")\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Feature importance error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea121aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
