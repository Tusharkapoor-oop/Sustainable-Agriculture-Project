{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0c56d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_results.py\n",
    "import os\n",
    "import joblib\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ef04bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Paths (works in script & Jupyter)\n",
    "# -----------------------------\n",
    "try:\n",
    "    # Case: running as a .py script\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "except NameError:\n",
    "    # Case: running inside Jupyter Notebook\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"data\", \"processed\", \"Crop_recommendation_clean.csv\")\n",
    "RAW_PATH = os.path.join(BASE_DIR, \"data\", \"raw\", \"Crop_recommendation.csv\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"outputs\", \"models\")\n",
    "BEST_MODEL_PATH = os.path.join(MODEL_DIR, \"best_model.pkl\")\n",
    "PLOTS_DIR = os.path.join(BASE_DIR, \"outputs\", \"reports\")\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9156417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Logging setup\n",
    "# -----------------------------\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(BASE_DIR, \"evaluation.log\"),\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "569abefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Load and Preprocess Data\n",
    "# -----------------------------\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load processed dataset or generate from raw if missing, with preprocessing.\"\"\"\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        if not os.path.exists(RAW_PATH):\n",
    "            raise FileNotFoundError(\"❌ Neither processed nor raw data found!\")\n",
    "        logger.warning(\"⚠️ Processed data not found. Using raw data instead.\")\n",
    "        df = pd.read_csv(RAW_PATH)\n",
    "        df.to_csv(DATA_PATH, index=False)\n",
    "    else:\n",
    "        df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "    # Separate features and target\n",
    "    if \"label\" not in df.columns:\n",
    "        raise KeyError(\"❌ Target column 'label' not found in dataset.\")\n",
    "    X = df.drop(\"label\", axis=1)\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    # Handle categorical features\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == \"object\":  # categorical column\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "            logger.info(f\"Encoded column: {col}\")\n",
    "\n",
    "    # Impute missing numeric values\n",
    "    imputer = SimpleImputer(strategy=\"mean\")  # can also use median\n",
    "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    logger.info(\"✅ Missing values imputed with mean\")\n",
    "\n",
    "    # Encode target if categorical\n",
    "    if y.dtype == \"object\":\n",
    "        le_y = LabelEncoder()\n",
    "        y = le_y.fit_transform(y.astype(str))\n",
    "        logger.info(\"Encoded target column 'label'\")\n",
    "\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "296237b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_select_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train multiple models and save the best one.\"\"\"\n",
    "    candidates = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "        \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42)\n",
    "    }\n",
    "\n",
    "    best_model, best_score, best_name = None, 0, None\n",
    "\n",
    "    for name, model in candidates.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        logger.info(f\"{name} accuracy: {acc:.4f}\")\n",
    "\n",
    "        if acc > best_score:\n",
    "            best_score, best_model, best_name = acc, model, name\n",
    "\n",
    "    joblib.dump(best_model, BEST_MODEL_PATH)\n",
    "    logger.info(f\"✅ Saved best model: {best_name} with accuracy {best_score:.4f}\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8467671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_train_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Load model if exists, else train a new one.\"\"\"\n",
    "    if os.path.exists(BEST_MODEL_PATH):\n",
    "        logger.info(\"📂 Loading existing best model...\")\n",
    "        return joblib.load(BEST_MODEL_PATH)\n",
    "    else:\n",
    "        logger.info(\"⚡ No pre-trained model found. Training new one...\")\n",
    "        return train_and_select_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b1f54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model performance with metrics and plots.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test) if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"f1\": f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "    # Save confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    cm_path = os.path.join(PLOTS_DIR, \"confusion_matrix.png\")\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Save ROC curve (only if binary classification)\n",
    "    if y_prob is not None and len(set(y_test)) == 2:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "        plt.plot([0, 1], [0, 1], \"r--\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC Curve\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        roc_path = os.path.join(PLOTS_DIR, \"roc_curve.png\")\n",
    "        plt.savefig(roc_path)\n",
    "        plt.close()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "824c5cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting evaluation pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tusha\\AppData\\Local\\Temp\\ipykernel_10048\\3609803861.py:17: DtypeWarning: Columns (7,11,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Metrics:\n",
      "Accuracy: 0.9999\n",
      "Precision: 0.9999\n",
      "Recall: 0.9999\n",
      "F1: 0.9999\n",
      "\n",
      "✅ Evaluation completed. Check 'outputs/reports/' and 'evaluation.log'.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Main Execution\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting evaluation pipeline...\")\n",
    "\n",
    "    # Load data\n",
    "    X_train, X_test, y_train, y_test = load_data()\n",
    "\n",
    "    # Load or train model\n",
    "    model = load_or_train_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    print(\"\\n📊 Evaluation Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k.capitalize()}: {v:.4f}\")\n",
    "\n",
    "    print(\"\\n✅ Evaluation completed. Check 'outputs/reports/' and 'evaluation.log'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a987ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Crop_recommendation.csv with shape (2200, 8)\n",
      "✅ Loaded data_core.csv with shape (8000, 9)\n",
      "✅ Loaded weatherHistory.csv with shape (96453, 12)\n",
      "✅ Combined dataframe shape: (96453, 25)\n",
      "✅ Preprocessing complete\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Sustainable Crop Recommendation - Robust Notebook\n",
    "# ===========================\n",
    "\n",
    "# 1️⃣ Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 2️⃣ Paths\n",
    "RAW_PATH = r\"C:\\Users\\tusha\\Downloads\\Sustainable-Crop-Recommendation\\data\\raw\"\n",
    "OUTPUT_PATH = r\"C:\\Users\\tusha\\Downloads\\Sustainable-Crop-Recommendation\\outputs\\models\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# 3️⃣ Load all CSV files safely\n",
    "files = [f for f in os.listdir(RAW_PATH) if f.endswith(\".csv\")]\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No CSV files found in raw folder!\")\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(RAW_PATH, f))\n",
    "        dfs.append(df)\n",
    "        print(f\"✅ Loaded {f} with shape {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load {f}: {e}\")\n",
    "\n",
    "# Merge datasets safely\n",
    "try:\n",
    "    combined_df = pd.concat(dfs, axis=1)\n",
    "    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n",
    "    print(f\"✅ Combined dataframe shape: {combined_df.shape}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error combining datasets: {e}\")\n",
    "\n",
    "# 4️⃣ Preprocessing with error handling\n",
    "try:\n",
    "    # Handle missing values\n",
    "    for col in combined_df.columns:\n",
    "        if combined_df[col].dtype in ['int64', 'float64']:\n",
    "            combined_df[col].fillna(combined_df[col].median(), inplace=True)\n",
    "        else:\n",
    "            combined_df[col].fillna(combined_df[col].mode()[0], inplace=True)\n",
    "\n",
    "    # Detect target column automatically (last column assumed)\n",
    "    target_col = combined_df.columns[-1]\n",
    "    if combined_df[target_col].isnull().all():\n",
    "        raise ValueError(\"Target column is completely empty!\")\n",
    "\n",
    "    y = combined_df[target_col]\n",
    "    if y.dtype == 'object':\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # Features\n",
    "    X = combined_df.drop(columns=[target_col])\n",
    "\n",
    "    # Encode categorical features\n",
    "    cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "    for col in cat_cols:\n",
    "        try:\n",
    "            X[col] = LabelEncoder().fit_transform(X[col])\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not encode {col}: {e}\")\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    print(\"✅ Preprocessing complete\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Preprocessing error: {e}\")\n",
    "\n",
    "# 5️⃣ Train-Test Split\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error in train-test split: {e}\")\n",
    "\n",
    "# 6️⃣ Model Training & Evaluation\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"SVM\": SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "best_acc = 0\n",
    "best_model_name = None\n",
    "best_model = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"--- {name} ---\")\n",
    "        print(\"Accuracy:\", acc)\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model_name = name\n",
    "            best_model = model\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error training {name}: {e}\")\n",
    "\n",
    "if best_model is None:\n",
    "    raise RuntimeError(\"No model trained successfully!\")\n",
    "print(f\"✅ Best Model: {best_model_name} with accuracy {best_acc:.4f}\")\n",
    "\n",
    "# Save model and scaler safely\n",
    "try:\n",
    "    joblib.dump(best_model, os.path.join(OUTPUT_PATH, \"best_model.pkl\"))\n",
    "    joblib.dump(scaler, os.path.join(OUTPUT_PATH, \"scaler.pkl\"))\n",
    "    print(\"✅ Model and scaler saved\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not save model/scaler: {e}\")\n",
    "\n",
    "# 7️⃣ Production Prediction Function with error handling\n",
    "def predict_new_data(new_data_df):\n",
    "    try:\n",
    "        # Fill missing values same as training\n",
    "        for col in new_data_df.columns:\n",
    "            if new_data_df[col].dtype in ['int64', 'float64']:\n",
    "                if col in combined_df.columns:\n",
    "                    new_data_df[col].fillna(combined_df[col].median(), inplace=True)\n",
    "                else:\n",
    "                    new_data_df[col].fillna(new_data_df[col].median(), inplace=True)\n",
    "            else:\n",
    "                if col in combined_df.columns:\n",
    "                    new_data_df[col].fillna(combined_df[col].mode()[0], inplace=True)\n",
    "                else:\n",
    "                    new_data_df[col].fillna(new_data_df[col].mode()[0], inplace=True)\n",
    "\n",
    "        # Encode categorical\n",
    "        for col in new_data_df.select_dtypes(include='object').columns:\n",
    "            if col in cat_cols:\n",
    "                le = LabelEncoder()\n",
    "                le.fit(combined_df[col])\n",
    "                new_data_df[col] = le.transform(new_data_df[col])\n",
    "            else:\n",
    "                new_data_df[col] = new_data_df[col].astype('category').cat.codes\n",
    "\n",
    "        # Scale\n",
    "        new_scaled = scaler.transform(new_data_df)\n",
    "\n",
    "        # Predict\n",
    "        preds = best_model.predict(new_scaled)\n",
    "        return preds\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Prediction error: {e}\")\n",
    "        return None\n",
    "\n",
    "# 8️⃣ Optional Feature Importance for Random Forest\n",
    "try:\n",
    "    if best_model_name == \"RandomForest\":\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "\n",
    "        feat_imp = pd.Series(best_model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(x=feat_imp.values, y=feat_imp.index)\n",
    "        plt.title(\"Feature Importance - Random Forest\")\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Feature importance error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd4ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
